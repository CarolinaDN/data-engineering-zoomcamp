{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a365252-83fe-478f-99b9-70dd0f4ddca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df377149-e294-456f-b98f-cfc67f01e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/02 16:28:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6778e484-6bf8-4d77-93c1-24bf0a6d08bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c59b7f5-03bc-4cde-836d-81e38afa3890",
   "metadata": {},
   "source": [
    "to implement but with rdd\n",
    "\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d73866e-b404-4b5b-a445-9eefd6318a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 27, 18, 7, 53), PULocationID=82, total_amount=12.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 17, 7, 38, 57), PULocationID=41, total_amount=7.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 30, 0, 40, 5), PULocationID=82, total_amount=7.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 29, 18, 59, 48), PULocationID=130, total_amount=16.64),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 11, 11, 56, 18), PULocationID=119, total_amount=39.56),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 22, 13, 0, 15), PULocationID=75, total_amount=10.79),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 19, 23, 14, 34), PULocationID=41, total_amount=6.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 22, 10, 48, 42), PULocationID=183, total_amount=14.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 15, 16, 34, 38), PULocationID=72, total_amount=16.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2019, 1, 31, 16, 24, 12), PULocationID=75, total_amount=8.75)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c73c778-3fd3-4495-b22c-66e6cc2d10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c10fb6-c8a5-4123-ba6c-42060a0f3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50000592-ae8d-446f-bb87-c1994f7ac44b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows = rdd.take(10)\n",
    "row = rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80f4ae7-d201-4544-b606-f832c275e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We need to generate intermediate results in a very similar way to the original SQL query, \n",
    "so we will need to create the composite key (hour, zone) and a composite value (amount, count),\n",
    "which are the 2 halves of each record that the executors will generate. \n",
    "Once we have a function that generates the record, we will use the map() method, which takes an RDD, \n",
    "transforms it with a function (our key-value function) and returns a new RDD. \n",
    "\"\"\"\n",
    "def prepare_for_grouping(row): \n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b962366-02e8-4d3e-8b2f-e2a1e0b0b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We now need to use the reduceByKey() method, which will take all records with the same key and put them together in a single record\n",
    "by transforming all the different values according to some rules which we can define with a custom function. \n",
    "Since we want to count the total amount and the total number of records, we just need to add the values: \n",
    "\"\"\"\n",
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e88c341-214d-4583-a60a-8bd07598c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2287a44d-cbbf-477e-9daa-3aafabc50c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will help with schema creation\n",
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2212b86-bde0-42b9-9933-21644df750dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The output we have is already usable but not very nice, so we map the output again in order to unwrap it. \n",
    "\"\"\"\n",
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57afb8d3-c650-41d6-8fca-e7f7a361cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0ccbbae-1315-4df4-93d3-0e9f0d457411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finally, we can take the resulting RDD and convert it to a dataframe with toDF(). \n",
    "We will need to generate a schema first because we lost it when converting RDDs:\n",
    "\"\"\"\n",
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e87ce7e6-f7f0-4dca-83be-c5d687e0a0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe can use toDF() without any schema as an input parameter, \\nbut Spark will have to figure out the schema by itself which may take a substantial amount of time. \\nUsing namedtuple in the previous step allows Spark to infer the column names but Spark will still need to figure out the data types;\\nby passing a schema as a parameter we skip this step and get the output much faster.\\n\\nAs you can see, manipulating RDDs to perform SQL-like queries is complex and time-consuming.\\nEver since Spark added support for dataframes and SQL, manipulating RDDs in this fashion has become obsolete,\\nbut since dataframes are built on top of RDDs, knowing how they work can help us understand how to make better use of Spark.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema) \n",
    "\n",
    "\"\"\"\n",
    "We can use toDF() without any schema as an input parameter, \n",
    "but Spark will have to figure out the schema by itself which may take a substantial amount of time. \n",
    "Using namedtuple in the previous step allows Spark to infer the column names but Spark will still need to figure out the data types;\n",
    "by passing a schema as a parameter we skip this step and get the output much faster.\n",
    "\n",
    "As you can see, manipulating RDDs to perform SQL-like queries is complex and time-consuming.\n",
    "Ever since Spark added support for dataframes and SQL, manipulating RDDs in this fashion has become obsolete,\n",
    "but since dataframes are built on top of RDDs, knowing how they work can help us understand how to make better use of Spark.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539ef9ab-8535-4e58-adc6-619f0331d3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242accc2-a7a1-48ee-bc4e-98ee5671815c",
   "metadata": {},
   "source": [
    "## mapPartitions()\n",
    "\n",
    "The mapPartitions() function behaves similarly to map() in how it receives an RDD as input and transforms it into another RDD \n",
    "with a function that we define but it transforms partitions rather than elements. \n",
    "In other words: map() creates a new RDD by transforming every single element, whereas mapPartitions() transforms every partition to create a new RDD.\n",
    "\n",
    "mapPartitions() is a convenient method for dealing with large datasets because it allows us to separate it into chunks \n",
    "that we can process more easily, which is handy for workflows such as Machine Learning.\n",
    "\n",
    "\n",
    "Example: service that predicts the duration of a trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c935b07c-3f95-4f8e-959b-79ddc44e51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00741d35-8d1f-4759-861f-334b2e4066ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1bbc860-33ec-41cb-a77b-68779e4500e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c66c722-b8ab-48d6-9c7f-5ea280dfd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ...\n",
    "\n",
    "def model_predict(df):\n",
    "#     y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e64011a9-1e0c-4b5e-b190-1ba5216d7ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf.itertuples() is an iterable that returns a tuple containing all the values in a single row, for all rows.\\nThus, row will contain a tuple with all the values for a single row.\\n\\nyield is a Python keyword that behaves similarly to return but returns a generator object instead of a value.\\nThis means that a function that uses yield can be iterated on. Spark makes use of the generator object in mapPartitions() to build the output RDD.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partitions can be unbalanced (some with more rows than others) and it may be necessary to handle but it's an expensive operation\n",
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df['predicted_duration'] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row\n",
    "\n",
    "\"\"\"\n",
    "df.itertuples() is an iterable that returns a tuple containing all the values in a single row, for all rows.\n",
    "Thus, row will contain a tuple with all the values for a single row.\n",
    "\n",
    "yield is a Python keyword that behaves similarly to return but returns a generator object instead of a value.\n",
    "This means that a function that uses yield can be iterated on. Spark makes use of the generator object in mapPartitions() to build the output RDD.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18e04cd-ad80-4d5f-bc43-9947476720d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch)\\\n",
    "    .toDF() \\\n",
    "    .drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0e0a4c9-9b33-4859-a2f6-6a5fead9e8e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|             14.55|\n",
      "|               5.4|\n",
      "|               6.2|\n",
      "|              12.5|\n",
      "|              43.6|\n",
      "|               7.9|\n",
      "|               4.1|\n",
      "|16.299999999999997|\n",
      "|             12.85|\n",
      "|               4.0|\n",
      "|              10.1|\n",
      "|              45.0|\n",
      "|               8.7|\n",
      "|               9.7|\n",
      "|17.450000000000003|\n",
      "|              4.45|\n",
      "|23.900000000000002|\n",
      "|             13.45|\n",
      "|              19.4|\n",
      "|               2.9|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51286)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/carolina/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/carolina/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/carolina/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/carolina/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/carolina/spark/spark-3.4.0-bin-hadoop3/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/carolina/spark/spark-3.4.0-bin-hadoop3/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"/home/carolina/spark/spark-3.4.0-bin-hadoop3/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/carolina/spark/spark-3.4.0-bin-hadoop3/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_predicts.select('predicted_duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077432c-0053-482a-9f9a-efaabb5f276f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
