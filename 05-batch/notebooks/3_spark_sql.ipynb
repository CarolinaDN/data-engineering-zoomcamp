{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebcaf80-09d3-409f-a850-cd3ec366396a",
   "metadata": {},
   "source": [
    "## Creating a Local Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00222855-8dc3-4098-bc6c-e03980e59e74",
   "metadata": {},
   "source": [
    "On this notebook, we are following the same strategy as in module 4 - dm_monthyl_zone_revenue.sql model - in Spark, to combine yellow and green data and create a trips_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b5b841-a1a6-4771-b452-728a90a55b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/02 16:31:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\nNote that we used the HTTP port 8080 for browsing to the dashboard but we use the Spark port 7077 for connecting our code to the cluster.\\nUsing localhost as a stand-in for the URL may not work.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName('test') \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#To create a Spark cluster in Standalone Mode so that the cluster can remain running even after we stop running our notebooks.\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://de-zoomcamp.europe-west1-b.c.vivid-grammar-424416-a0.internal:7077\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "Note that we used the HTTP port 8080 for browsing to the dashboard but we use the Spark port 7077 for connecting our code to the cluster.\n",
    "Using localhost as a stand-in for the URL may not work.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5c3bb-d09d-49e9-9f86-4bdbf5bcf910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcedfdd-c98a-46ff-be5c-e761ceb36385",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_green' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Because the pickup and dropoff column names don't match between the 2 datasets, we use the withColumnRenamed action to make them have matching names.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_green \u001b[38;5;241m=\u001b[39m df_green \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlpep_pickup_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlpep_dropoff_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropoff_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_green' is not defined"
     ]
    }
   ],
   "source": [
    "#Because the pickup and dropoff column names don't match between the 2 datasets, we use the withColumnRenamed action to make them have matching names.\n",
    "\n",
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6032d4-eece-45c9-916b-fc642a513ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c90d8-68e1-40be-b3ae-3d04bc08d068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d6079-a262-4834-8f71-c5725370b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('data/pq/yellow/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b032ae7-993e-4eb0-8c50-38bfb9ef5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94541f33-6ba8-477e-a228-33a3f5f12f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(df_green.columns) & set(df_yellow.columns)\n",
    "# instead of set to preserve order\n",
    "common_colums = []\n",
    "\n",
    "yellow_columns = set(df_yellow.columns)\n",
    "\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_colums.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6e708-e4f3-42fc-a272-a46f7a685566",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e8ae2c-6946-4057-bb50-7dc8ac0b2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f37db82-8416-4017-a4ec-9460c474c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only common columns\n",
    "# new column with consant value green/yellow - keep track of the taxi type for each record\n",
    "df_green_sel = df_green \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('green'))\n",
    "\n",
    "df_yellow_sel = df_yellow \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e003917-7e57-4bf5-b1a3-59da294b3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data = df_green_sel.unionAll(df_yellow_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abd3827e-1119-48bf-b13f-cd5455f388c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (39 + 1) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|service_type|    count|\n",
      "+------------+---------+\n",
      "|       green|  8348567|\n",
      "|      yellow|124048218|\n",
      "+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_trips_data.groupBy('service_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134cb1cb-1672-4f26-9c43-38213b9d6d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge',\n",
       " 'service_type']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60f378d1-cb3f-4374-aa66-f5e00b81703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolina/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#SQL expects a table for retrieving records, but a dataframe is not a table, so we need to register the dataframe as a table first:\n",
    "#This method creates a temporary table with the name trips_data.\n",
    "#With our registered table, we can now perform regular SQL operations.\n",
    "df_trips_data.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a74af46c-394c-451d-82ca-39e199f59745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:================================================>        (34 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|congestion_surcharge|service_type|\n",
      "+--------+-------------------+-------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "|       2|2019-01-27 18:07:53|2019-01-27 18:20:54|                 N|         1|          82|         160|              1|         2.91|       11.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        12.3|           2|                null|       green|\n",
      "|       2|2019-01-17 07:38:57|2019-01-17 07:46:47|                 N|         1|          41|          74|              1|         1.08|        7.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         7.8|           2|                null|       green|\n",
      "|       2|2019-01-30 00:40:05|2019-01-30 00:45:53|                 N|         1|          82|         129|              1|         1.24|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|           2|                 0.0|       green|\n",
      "|       2|2019-01-29 18:59:48|2019-01-29 19:10:56|                 N|         1|         130|          10|              1|          2.5|       11.0|  1.0|    0.5|      3.84|         0.0|                  0.3|       16.64|           1|                 0.0|       green|\n",
      "|       2|2019-01-11 11:56:18|2019-01-11 12:37:23|                 N|         1|         119|         226|              1|         8.72|       33.0|  0.0|    0.5|       0.0|        5.76|                  0.3|       39.56|           1|                null|       green|\n",
      "|       2|2019-01-22 13:00:15|2019-01-22 13:07:14|                 N|         1|          75|         151|              1|         1.58|        7.5|  0.0|    0.5|      2.49|         0.0|                  0.3|       10.79|           1|                null|       green|\n",
      "|       2|2019-01-19 23:14:34|2019-01-19 23:19:17|                 N|         1|          41|          42|              1|         0.82|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|           2|                null|       green|\n",
      "|       2|2019-01-22 10:48:42|2019-01-22 11:03:54|                 N|         1|         183|          81|              1|         3.26|       13.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        14.3|           1|                null|       green|\n",
      "|       2|2019-01-15 16:34:38|2019-01-15 16:54:31|                 N|         1|          72|          61|              1|         2.57|       14.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        16.3|           1|                null|       green|\n",
      "|       1|2019-01-31 16:24:12|2019-01-31 16:29:21|                 N|         1|          75|         236|              1|          0.8|        5.5|  1.0|    0.5|      1.45|         0.0|                  0.3|        8.75|           1|                 0.0|       green|\n",
      "+--------+-------------------+-------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM trips_data limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e036cb3a-10e6-417a-aaf4-128565a6d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|service_type| count(1)|\n",
      "+------------+---------+\n",
      "|       green|  8348567|\n",
      "|      yellow|124048218|\n",
      "+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    service_type,\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY \n",
    "    service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be27eb8-911a-4167-a7c5-789b85b6cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now slightly modify the dm_monthyl_zone_revenue.sql, and run it as a query with Spark and store the output in a dataframe:\n",
    "\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d0b5093-4bbb-43e7-a492-d89d11fdc089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df_result.write.parquet('data/report/revenue/')\n",
    "#With our current dataset, this will create more than 200 parquet files of very small size, which isn't very desirable.\n",
    "\n",
    "#In order to reduce the amount of files, we need to reduce the amount of partitions of the dataset, which is done with the coalesce() method:\n",
    "#This reduces the amount of partitions to just 1.\n",
    "df_result.coalesce(1).write.parquet('data/report/revenue/', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
